# Search Engine with TF-IDF and BERT

Эта работа представляет собой простую поисковую систему, которая позволяет выполнять поиск по текстам с использованием двух методов индексации: TF-IDF и эмбеддингов BERT. Система поддерживает предварительное вычисление индексов и сохранение их в файлы, чтобы избежать повторных расчетов.

## Описание

### 1. Корпус данных

В проекте используется корпус текстов из русской Википедии с сайта HuggingFace. Он состоит из 6 тысяч статей, что является хорошей основой для демонстрации работы поисковой системы на русском языке.

### 2. Предобработка текста

Тексты проходят следующие этапы обработки перед индексацией:
- Приведение к нижнему регистру
- Удаление пунктуации
- Лемматизация слов с использованием библиотеки `pymorphy2`
- Удаление стоп-слов с использованием библиотеки `nltk`

### 3. Индексация

Реализованы два способа индексации:
- **TF-IDF** : Преобразует тексты в числовые векторы на основе частоты слов в документах и их редкости в корпусе. Для индексации используется `TfidfVectorizer` из библиотеки `scikit-learn`.
- **BERT** : Модель `ruBERT` от `DeepPavlov` используется для генерации эмбеддингов предложений. Она обучена на русскоязычных текстах и токенизирует текст перед преобразованием в эмбеддинги.

### 4. Поиск

Система поддерживает поиск по запросу с использованием выбранного метода индексации (TF-IDF или BERT). Поиск основан на косинусном сходстве между запросом и векторами документов.

## Установка

1. Убедитесь, что у вас установлен **Python 3.8+**.
2. Установите необходимые библиотеки с помощью команды:

```bash
pip install pandas nltk pymorphy2 scikit-learn torch transformers numpy pickle-mixin
```

3. Загрузите стоп-слова NLTK:

```python
import nltk
nltk.download('stopwords')
```

## Подготовка данных

Перед запуском поиска необходимо выполнить индексацию текстов.

### 1. Загрузка и предобработка корпуса:

```python
wiki_df = pd.read_json("hf://datasets/Den4ikAI/russian_cleared_wikipedia/wiki_dataset.json", lines=True)
```

Затем примените предобработку текста:

```python
wiki_df['cleaned_text'] = wiki_df['sample'].apply(preprocess_text)
```

### 2. Индексация TF-IDF:

```python
tfidf_vectorizer = TfidfVectorizer()
tfidf_matrix = tfidf_vectorizer.fit_transform(wiki_df['cleaned_text'])

with open('tfidf_vectorizer.pkl', 'wb') as f:
    pickle.dump(tfidf_vectorizer, f)
with open('tfidf_matrix.pkl', 'wb') as f:
    pickle.dump(tfidf_matrix, f)
```

### 3. Индексация BERT:

```python
bert_embeddings = get_bert_embeddings(wiki_df['cleaned_text'])

with open('bert_embeddings.pkl', 'wb') as f:
    pickle.dump(bert_embeddings, f)
```

## Запуск поиска

Запуск осуществляется через командную строку. Введите запрос и выберите метод индексации (`tf-idf` или `bert`):

```bash
python search_engine.py
```

### Пример использования:

```markdown
Введите текст запроса: Пример запроса
Введите, какой вариант индекса использовать ('tf-idf' или 'bert'): tf-idf
```

Система вернет список из 5 наиболее релевантных документов с указанием степени их сходства с запросом.

## Структура файлов

```
.
├── README.md                 # Этот файл
├── search_engine.py          # Основной скрипт с логикой поиска
├── tfidf_vectorizer.pkl      # Предобученный TF-IDF векторизатор
├── tfidf_matrix.pkl          # Матрица TF-IDF
├── bert_embeddings.pkl       # Эмбеддинги BERT
```